{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import the needed packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manderson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manderson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import keras\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows : 2225\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>Singer Sizzla jailed for swearing\\n \\n Reggae ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>Halo 2 sells five million copies\\n \\n Microsof...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>Spears seeks aborted tour payment\\n \\n Singer ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>Blair 'up for it' ahead of poll\\n \\n Tony Blai...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>Net fingerprints combat attacks\\n \\n Eighty la...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Man Utd to open books to Glazer\\n \\n Mancheste...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>BAA support ahead of court battle\\n \\n UK airp...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>Director Nair's Vanity project\\n \\n Indian fil...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>De Niro film leads US box office\\n \\n Film sta...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>Brown targets OAPs and homebuyers\\n \\n Gordon ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   news           type\n",
       "741   Singer Sizzla jailed for swearing\\n \\n Reggae ...  entertainment\n",
       "1893  Halo 2 sells five million copies\\n \\n Microsof...           tech\n",
       "709   Spears seeks aborted tour payment\\n \\n Singer ...  entertainment\n",
       "1210  Blair 'up for it' ahead of poll\\n \\n Tony Blai...       politics\n",
       "1915  Net fingerprints combat attacks\\n \\n Eighty la...           tech\n",
       "167   Man Utd to open books to Glazer\\n \\n Mancheste...       business\n",
       "1159  BAA support ahead of court battle\\n \\n UK airp...       politics\n",
       "855   Director Nair's Vanity project\\n \\n Indian fil...  entertainment\n",
       "593   De Niro film leads US box office\\n \\n Film sta...  entertainment\n",
       "1001  Brown targets OAPs and homebuyers\\n \\n Gordon ...       politics"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- The BBC dataset\n",
    "news_data = pd.read_csv('articles.csv')\n",
    "\n",
    "#-- let's look at some of the data\n",
    "print(\"Number of rows : \" + str(news_data.shape[0]))\n",
    "news_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clean our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "def clean_line(t):\n",
    "    return (t.replace(' \\n ',' ')\n",
    "            .replace('\\r',' ')\n",
    "            .replace('\\t',' ')\n",
    "            .replace('  ',' ')\n",
    "            .strip().lower())\n",
    "\n",
    "#-- initialize the lemmatizer\n",
    "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
    "from nltk import tokenize\n",
    "\n",
    "paras = []\n",
    "labels = []\n",
    "texts = []\n",
    "for raw_doc in news_data['news']:\n",
    "    text = clean_line(raw_doc)\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    paras.append(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TD-IDF Weight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>000bn</th>\n",
       "      <th>000m</th>\n",
       "      <th>000s</th>\n",
       "      <th>000th</th>\n",
       "      <th>001</th>\n",
       "      <th>001and</th>\n",
       "      <th>001st</th>\n",
       "      <th>...</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zornotza</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zubair</th>\n",
       "      <th>zuluaga</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zutons</th>\n",
       "      <th>zvonareva</th>\n",
       "      <th>zvyagintsev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 29421 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00       000  0001  000bn  000m  000s  000th  001  001and  001st  ...  \\\n",
       "2165  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "1579  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "128   0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "2104  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "2084  0.0  0.021565   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "2195  0.0  0.013586   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "1704  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "709   0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "1867  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "2044  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "\n",
       "      zooms  zooropa  zornotza  zorro  zubair  zuluaga  zurich  zutons  \\\n",
       "2165    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "1579    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "128     0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "2104    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "2084    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "2195    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "1704    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "709     0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "1867    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "2044    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "\n",
       "      zvonareva  zvyagintsev  \n",
       "2165        0.0          0.0  \n",
       "1579        0.0          0.0  \n",
       "128         0.0          0.0  \n",
       "2104        0.0          0.0  \n",
       "2084        0.0          0.0  \n",
       "2195        0.0          0.0  \n",
       "1704        0.0          0.0  \n",
       "709         0.0          0.0  \n",
       "1867        0.0          0.0  \n",
       "2044        0.0          0.0  \n",
       "\n",
       "[10 rows x 29421 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#-- function to create document term matrix\n",
    "def createDTM(text):\n",
    "    vect = TfidfVectorizer()\n",
    "    dtm = vect.fit_transform(text)\n",
    "    #-- create pandas dataframe of DTM\n",
    "    return pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "#-- let's take a look at the dtm\n",
    "documents = createDTM(news_data['news'])\n",
    "documents.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GLoVe Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "WORD_SIZE = 300\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(os.getcwd(), 'glove.840B.300d.txt'), encoding='UTF-8')\n",
    "\n",
    "for line in f:\n",
    "    tokens = line.split(\" \")\n",
    "    if tokens:\n",
    "        embeddings_index[tokens[0]] = np.asarray(tokens[1:WORD_SIZE + 1], dtype='float32')\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessng with Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_NUM = 9\n",
    "MAX_WORD_NUM = 40\n",
    "MAX_FEATURES = 200000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES, oov_token=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# create raw data by mapping each word to its embedding*tf-idf weighting\n",
    "data = np.zeros((len(texts), MAX_SENTENCE_NUM, MAX_WORD_NUM, WORD_SIZE), dtype='float32')\n",
    "for i, document in enumerate(paras):\n",
    "    wordCount = 0\n",
    "    for j, sentence in enumerate(document):\n",
    "        if j < MAX_SENTENCE_NUM:\n",
    "            words = text_to_word_sequence(sentence)\n",
    "            for k, word in enumerate(words):\n",
    "                wordCount += 1\n",
    "                if k < MAX_WORD_NUM:\n",
    "                    word_emedding = np.zeros((WORD_SIZE), dtype='float32')\n",
    "                    if word in documents.columns and word in embeddings_index:\n",
    "                        word_weight = documents[word].iloc[i] if word in documents.columns else 1\n",
    "                        word_emedding = embeddings_index[word] * word_weight\n",
    "                    data[i,j,k] = word_emedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2225, 9, 40, 300)\n",
      "Shape of topics tensor: (2225, 5)\n"
     ]
    }
   ],
   "source": [
    "topics = pd.get_dummies(news_data['type'])\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of topics tensor:', topics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation split\n",
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "topics = topics.iloc[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = topics[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = topics[-nb_validation_samples:]\n",
    "\n",
    "actual_labels = topics.idxmax(axis = 1).map({'Business': 0, 'Entertainment': 1, 'Politics': 2, 'Sports' : 3, 'Technology' : 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Attention Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Hierarchial Attention Layer as described by Hierarchical Attention Networks for Document Classification(2016)\n",
    "    - Yang et. al.\n",
    "    Source: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "    Theano backend\n",
    "    \"\"\"\n",
    "    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
    "        # Initializer \n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Builds all weights\n",
    "        # W = Weight matrix, b = bias vector, u = context vector\n",
    "        assert len(input_shape) == 3\n",
    "        # casting \"Dimension\" type of input_shape's elements to \"int\" type\n",
    "        #input_shape = tuple([i if (isinstance(i, float) or i is None else i.value for i in input_shape)])\n",
    "        with tf.init_scope():\n",
    "            self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
    "            self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "            self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self._trainable_weights = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_input  Tensor(\"sent_input:0\", shape=(None, 9, 40, 300), dtype=float32)\n",
      "<tensorflow.python.keras.engine.training.Model object at 0x000001E4E104B978>\n",
      "-------------------\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sent_input (InputLayer)      [(None, 9, 40, 300)]      0         \n",
      "_________________________________________________________________\n",
      "sent_linking (TimeDistribute (None, 9, 100)            146000    \n",
      "_________________________________________________________________\n",
      "sent_gru (Bidirectional)     (None, 9, 100)            45300     \n",
      "_________________________________________________________________\n",
      "sent_dense (Dense)           (None, 9, 100)            10100     \n",
      "_________________________________________________________________\n",
      "sent_attention (AttentionLay [(None, 100), (None, 9, 1 30600     \n",
      "_________________________________________________________________\n",
      "sent_dropout (Dropout)       (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 232,505\n",
      "Trainable params: 232,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.layers import Bidirectional, GRU, Dense\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras.layers import TimeDistributed\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "\n",
    "# Words level attention model\n",
    "word_input = Input(shape=(MAX_WORD_NUM, WORD_SIZE, ), dtype='float32', name='word_input')\n",
    "word_gru = Bidirectional(GRU(50, return_sequences=True),name='word_gru')(word_input)\n",
    "word_dense = Dense(100, activation='relu', name='word_dense')(word_gru) \n",
    "word_att, word_coeffs = AttentionLayer(WORD_SIZE, True, name='word_attention')(word_dense)\n",
    "wordEncoder = Model(inputs = word_input,outputs = word_att)\n",
    "\n",
    "\n",
    "# Sentence level attention model\n",
    "sent_input = Input(shape=(MAX_SENTENCE_NUM, MAX_WORD_NUM, WORD_SIZE), dtype='float32',name='sent_input')\n",
    "print(\"sent_input \",sent_input)\n",
    "sent_encoder = TimeDistributed(wordEncoder, name='sent_linking')(sent_input)\n",
    "sent_gru = Bidirectional(GRU(50, return_sequences=True),name='sent_gru')(sent_encoder)\n",
    "sent_dense  = Dense(100, activation='relu', name='sent_dense')(sent_gru) \n",
    "sent_att,sent_coeffs = AttentionLayer(EMBED_SIZE,return_coefficients=True,name='sent_attention')(sent_dense)\n",
    "sent_drop = Dropout(0.5,name='sent_dropout')(sent_att)\n",
    "preds = Dense(5, activation='softmax',name='output')(sent_drop)\n",
    "\n",
    "# Model compile\n",
    "model = Model(sent_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "print(wordEncoder)\n",
    "print(\"-------------------\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1780 samples, validate on 445 samples\n",
      "Epoch 1/50\n",
      " 800/1780 [============>.................] - ETA: 16s - loss: 1.6085 - acc: 0.2100"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=50, batch_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
